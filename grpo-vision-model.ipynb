{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f77a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.57.0\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d7eff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.4: Fast Qwen3_Vl patching. Transformers: 4.57.0.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0382550a5f7a44998bab4977740062ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc525321ca04f28b03fe94c26f651fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/213 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf00463b8f7d4c1cada26aafe7e75a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/782 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3736967eb9af40e4b00acaa4f59e01ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3d3a4b94af4c63b834b87345591fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cbe5dff5d441058f4f83f3327b5626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6974a3c9d1943ecb70597a7c5d8e212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aafcc61da454a01a75ad05ef697eeca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c2bf08c39d440395ca02f6a0b89517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa7d70cdabf43a390bb5d14f03b7540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c988e264ec20442681fc8e59e8821000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "video_preprocessor_config.json:   0%|          | 0.00/817 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "max_seq_length = 16384\n",
    "lora_rank = 16\n",
    "model = FastVisionModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=False,\n",
    "    gpu_memory_utilization=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c04d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, processor = model  # Unpack the tuple first\n",
    "\n",
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = False,\n",
    "    finetune_language_layers   = True,\n",
    "    finetune_attention_modules = True,\n",
    "    finetune_mlp_modules       = True,\n",
    "\n",
    "    r = lora_rank,\n",
    "    lora_alpha = lora_rank,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "685ff3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425e1b9f521e4dd29f24f15a57a3fdf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896f294771bf47229be556679fd1622c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/testmini-00000-of-00001-725687bf7a1(…):   0%|          | 0.00/142M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668224a678fa414e80a00600d7e7c0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00002-6b81bd7f7e2065e(…):   0%|          | 0.00/358M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38581713746840db942717bc95245684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00001-of-00002-6a611c71596db30(…):   0%|          | 0.00/386M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a919659836844603b181567d8ecc691e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating testmini split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77bcf5105294a39a5c8c0675d89c514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "dataset = load_dataset(\"AI4Math/MathVista\", split = \"testmini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebe439ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e924454dcf4d2680882c53c3ace0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def is_numeric_answer(example):\n",
    "    try:\n",
    "        float(example[\"answer\"])\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "dataset = dataset.filter(is_numeric_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6184f3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f26fd133dfa4a0da3e8e3a7be3cb1a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbddad1a60ba4b20b27305150a8409b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def resize_image(example):\n",
    "    image=example[\"decoded_image\"]\n",
    "    image=image.resize((512,512))\n",
    "    example[\"decoded_image\"] = image\n",
    "    return example\n",
    "dataset = dataset.map(resize_image)\n",
    "\n",
    "def convert_to_rgb(example):\n",
    "    image=example[\"decoded_image\"]\n",
    "    if image.mode != \"RGB\":\n",
    "        image=image.convert(\"RGB\")\n",
    "    example[\"decoded_image\"] = image\n",
    "    return example\n",
    "dataset = dataset.map(convert_to_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58047c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b29c49581d843fd955a2cfd84703d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "REASONING_START='<REASONING>'\n",
    "REASONING_END='</REASONING>'\n",
    "SOLUTION_START='<SOLUTION>'\n",
    "SOLUTION_END='</SOLUTION>'\n",
    "def make_conversation(example):\n",
    "    text_content=(\n",
    "        f\"{example['question']}. Also first provide your reasoning or working out\"\\\n",
    "        f\" on how you would go about solving the question between {REASONING_START} and {REASONING_END}\"\n",
    "        f\" and then your final answer between {SOLUTION_START} and (put a single float here) {SOLUTION_END}\"\n",
    "    )\n",
    "    prompt=[{\"role\":\"user\",\"content\":[{\"type\":\"image\"},{\"type\":\"text\",\"text\":text_content}]}]\n",
    "    return {\"prompt\":prompt,\"image\":example[\"decoded_image\"], \"answer\":example[\"answer\"]}\n",
    "train_dataset = dataset.map(make_conversation)\n",
    "train_dataset=train_dataset.remove_columns('image')\n",
    "train_dataset=train_dataset.rename_column('decoded_image','image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9b4913f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04cfdcb3a25444385795ab9aa638b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/566 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(\n",
    "    lambda example: {\n",
    "        \"prompt\": processor.tokenizer.apply_chat_template(\n",
    "            example[\"prompt\"],\n",
    "            tokenize = False,\n",
    "            add_generation_prompt = True,\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b7b1573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def formatting_reward_func(completions,**kwargs):\n",
    "    thinking_pattern = re.compile(f\"{REASONING_START}(.*?){REASONING_END}\", re.DOTALL)\n",
    "    solution_pattern = re.compile(f\"{SOLUTION_START}(.*?){SOLUTION_END}\", re.DOTALL)\n",
    "    scores=[]\n",
    "    for completion in completions:\n",
    "        score=0\n",
    "        thinking_match = re.findall(thinking_pattern, completion)\n",
    "        solution_match = re.findall(solution_pattern, completion)\n",
    "        if len(thinking_match)==1:\n",
    "            score+=1.0\n",
    "        if len(solution_match)==1:\n",
    "            score+=1.0\n",
    "        if len(completion)!=0:\n",
    "            removal=completion.replace(\"addCriterion\",\"\").replace(\"\\n\",\"\")\n",
    "            if(len(completion)-len(removal))/len(completion) >= 0.5:\n",
    "                score-=2.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "                \n",
    "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
    "    answer_pattern = f'{SOLUTION_START}(.*?){SOLUTION_END}'\n",
    "\n",
    "    responses = [re.findall(answer_pattern, completion, re.DOTALL) for completion in completions]\n",
    "    q = prompts[0]\n",
    "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:{completions[0]}\")\n",
    "    return [\n",
    "        2.0 if len(r)==1 and a == r[0].replace('\\n','') else 0.0\n",
    "        for r, a in zip(responses, answer)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9b051bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>When a spring does work on an object, we cannot find the work by simply multiplying the spring force by the object's displacement. The reason is that there is no one value for the force-it changes. However, we can split the displacement up into an infinite number of tiny parts and then approximate the force in each as being constant. Integration sums the work done in all those parts. Here we use the generic result of the integration.\\r\\n\\r\\nIn Figure, a cumin canister of mass $m=0.40 \\\\mathrm{~kg}$ slides across a horizontal frictionless counter with speed $v=0.50 \\\\mathrm{~m} / \\\\mathrm{s}$. It then runs into and compresses a spring of spring constant $k=750 \\\\mathrm{~N} / \\\\mathrm{m}$. When the canister is momentarily stopped by the spring, by what distance $d$ is the spring compressed?. Also first provide your reasoning or working out on how you would go about solving the question between <REASONING> and </REASONING> and then your final answer between <SOLUTION> and (put a single float here) </SOLUTION><|im_end|>\\n<|im_start|>assistant\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a42c13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Move the ruler to measure the length of the nail to the nearest inch. The nail is about (_) inches long.. Also first provide your reasoning or working out on how you would go about solving the question between <REASONING> and </REASONING> and then your final answer between <SOLUTION> and (put a single float here) </SOLUTION><|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[100][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "817775d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<REASONING>\n",
      "To measure the length of the nail to the nearest inch, I need to determine how long the nail is by comparing it to the ruler.\n",
      "\n",
      "Step 1: Identify the starting point of the nail.\n",
      "The head of the nail (the circular part) is aligned with the 0-inch mark on the ruler.\n",
      "\n",
      "Step 2: Identify the ending point of the nail.\n",
      "The sharp tip of the nail extends past the 3-inch mark but does not reach the 4-inch mark.\n",
      "\n",
      "Step 3: Determine the length to the nearest inch.\n",
      "Since the tip is between 3 and 4 inches, and we are rounding to the nearest inch, I need to see which whole number it is closer to. The tip is clearly past 3 and before 4, so I need to estimate if it's closer to 3 or 4.\n",
      "\n",
      "Looking at the visual: the tip appears to be just shy of the 4-inch mark. It’s about 3.5 inches long (since it’s roughly halfway between 3 and 4), but since we need to round to the nearest inch, 3.5 rounds up to 4. However, let’s be precise. The nail's tip looks to be at approximately 3.2 to 3.4 inches. Since this is less than 3.5, it rounds down to 3 inches.\n",
      "\n",
      "But wait — let’s check again. The question says \"to the nearest inch\". The nail’s tip is clearly past 3 and is very close to 4, but not quite there. If it were exactly 3.5 inches, it would round to 4. But visually, it’s not 3.5 — it’s less than that. It appears to be about 3.3 inches. 3.3 is closer to 3 than to 4, so it rounds to 3.\n",
      "\n",
      "However, looking at the image more carefully, the tip appears to be just barely past the 3-inch mark and seems to be very close to the 4-inch mark — perhaps even a bit over 3.5. But since we are rounding to the nearest inch, we look at the midpoint between 3 and 4, which is 3.5. If the nail’s length is 3.5 inches or more, we round up to 4. If it’s less than 3.5, we round down to 3.\n",
      "\n",
      "In the image, the nail’s tip appears to be past 3.5. Let me estimate: from 3 to 4, the ruler is marked in 0.5 inch increments (since each major tick is 1 inch, and the midpoints are marked implicitly). The tip appears to be just before the 4-inch mark. So if it’s at 3.8 inches, that’s closer to 4 than to 3, so it rounds to 4. But if it’s at 3.2, it rounds to 3.\n",
      "\n",
      "Looking at the image, the tip is past the 3-inch mark and appears to be just shy of the 4-inch mark — maybe 3.7 or 3.8 inches. Since 3.7 and 3.8 are both greater than 3.5, they round up to 4.\n",
      "\n",
      "Therefore, to the nearest inch, the nail is 4 inches long.\n",
      "\n",
      "But wait — let’s be precise. Let me imagine the ruler: 0, 1, 2, 3, 4, 5, 6. The nail’s head is at 0. The tip is clearly beyond 3 and appears to be very close to 4, but not quite there. Is it 3.5? No, it’s less than 3.5? No — if it were 3.5, it would be exactly halfway. Since it’s visually past 3.5 (because it’s not at 3.5, and it’s getting close to 4), it should round to 4.\n",
      "\n",
      "Actually, I need to be careful. Let me measure it. From 0 to 3 is 3 inches. The tip is past 3, so it’s more than 3. If it were 3.1, it would round to 3. If it were 3.6, it would round to 4. The nail in the image appears to be approximately 3.6 to 3.7 inches long. Since 3.6 > 3.5, it rounds to 4.\n",
      "\n",
      "So, to the nearest inch, the nail is 4 inches long.\n",
      "\n",
      "</REASONING>\n",
      "<SOLUTION>4.0</SOLUTION><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "image = train_dataset[100][\"image\"]\n",
    "prompt = train_dataset[100][\"prompt\"]\n",
    "\n",
    "inputs = processor(\n",
    "    images=image,\n",
    "    text=prompt,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(processor.tokenizer, skip_prompt=True)\n",
    "_ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=1024,\n",
    "                   use_cache=True, temperature=1.0, min_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdef223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We now expect `per_device_train_batch_size` * `gradient_accumulation_steps` * `world_size` to be a multiple of `num_generations`.\n",
      "We will change the batch size of 1 to the `num_generations` of 2\n"
     ]
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer\n",
    "training_args = GRPOConfig(\n",
    "    learning_rate = 5e-6,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.99,\n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    log_completions = False,\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Increase to 4 for smoother training\n",
    "    num_generations = 2, # Decrease if out of memory\n",
    "    max_prompt_length = 1024,\n",
    "    max_completion_length = 1024,\n",
    "    num_train_epochs = 0.1, # Set to 1 for a full training run\n",
    "    max_steps = 60,\n",
    "    save_steps = 60,\n",
    "    max_grad_norm = 0.1,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    "\n",
    "    # Below enables GSPO:\n",
    "    importance_sampling_level = \"sequence\",\n",
    "    mask_truncated_completions = False,\n",
    "    loss_type = \"dr_grpo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827eedcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 566 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n",
      " \"-____-\"     Trainable parameters = 43,646,976 of 8,810,770,672 (0.50% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Question:\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|>What is the highest value on the X axis?. Also first provide your reasoning or working out on how you would go about solving the question between <REASONING> and </REASONING> and then your final answer between <SOLUTION> and (put a single float here) </SOLUTION><|im_end|>\n",
      "<|im_start|>assistant\n",
      " \n",
      "Answer:\n",
      "30 \n",
      "Response:<REASONING>\n",
      "To determine the highest value on the X-axis of the given graph, I need to examine the horizontal axis (X-axis) which is labeled \"MICROGRAMS/ml-E-DNP-LYSINE-HCL\". The X-axis has tick marks with labeled values: 0, 5, 10, 15, 20, 25, and 30. The last tick mark on the right side of the axis is labeled \"30\". Therefore, the highest value shown on the X-axis is 30.\n",
      "\n",
      "</REASONING>\n",
      "<SOLUTION>30.0</SOLUTION>\n",
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3/60 01:03 < 1:00:39, 0.02 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / formatting_reward_func / mean</th>\n",
       "      <th>rewards / formatting_reward_func / std</th>\n",
       "      <th>rewards / correctness_reward_func / mean</th>\n",
       "      <th>rewards / correctness_reward_func / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>155.500000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>155.500000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Question:\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|>What is the age gap between these two people in image?. Also first provide your reasoning or working out on how you would go about solving the question between <REASONING> and </REASONING> and then your final answer between <SOLUTION> and (put a single float here) </SOLUTION><|im_end|>\n",
      "<|im_start|>assistant\n",
      " \n",
      "Answer:\n",
      "6 \n",
      "Response:<REASONING>\n",
      "To determine the age gap between the two people in the image, I would need to analyze visual cues such as facial features, body proportions, hairstyle, clothing style, and context (e.g., era of the photo). However, this is a black-and-white photograph, and without clear identifiers like birth dates, names, or other contextual information, it is impossible to determine their exact ages or the difference between them with any degree of certainty.\n",
      "\n",
      "In such cases, we must consider whether there is any other information provided in the image that might help. The image shows two men standing next to a makeshift structure with a sign that appears to read \"MAN\" or \"MANG\". There is no visible text, date, or other metadata to indicate their ages or the time period. Even if they appear to be of similar age, without quantitative data, we cannot calculate an age gap.\n",
      "\n",
      "Since the question asks for a numerical age gap, and no such information is available in the image or in the prompt, the only logically consistent answer is that the age gap cannot be determined from the given information.\n",
      "\n",
      "Therefore, the age gap is unknown and cannot be calculated.\n",
      "\n",
      "</REASONING>\n",
      "\n",
      "<SOLUTION>0.0</SOLUTION>\n"
     ]
    }
   ],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    # Pass the processor to handle multimodal inputs\n",
    "    processing_class = processor,\n",
    "    reward_funcs = [\n",
    "        formatting_reward_func,\n",
    "        correctness_reward_func,\n",
    "    ],\n",
    "    train_dataset = train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a9129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7089f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
