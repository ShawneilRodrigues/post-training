{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7cd1ed",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install required packages for Unsloth and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "914ada8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb0e06",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Load the Gemma-3 270M model using Unsloth's FastModel for efficient inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2225abba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.11.4: Fast Gemma3 patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3327e5568a7b4ca4a2ab03712b372f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/536M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883995b54a844f89ab81caf04dcc43ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1b09f999a4446a915815ea5dbc5f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfe1fdd8019e443cb94b7bf1a527566e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32b0fa52fc2402c9387e3d3c9f9c4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5668bd02a7c4d7fbe3e5f6b330d7fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3405e4addb4f978b787f639956354d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/670 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba4a2e235c741968e830a1c2422f99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048  # Sufficient for GSM8K problems\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-3-270m-it\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,   # Full precision for small model\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,  # Use LoRA for efficiency\n",
    "    # token = \"hf_...\",  # Uncomment if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a5b7b4",
   "metadata": {},
   "source": [
    "## Configure LoRA Adapters\n",
    "\n",
    "Add LoRA adapters for parameter-efficient fine-tuning. This allows us to update only a small fraction of the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a57e6b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r=128,  # LoRA rank - higher values = more capacity but more memory\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0,  # No dropout for optimized training\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # 30% less VRAM\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e7ac4",
   "metadata": {},
   "source": [
    "## Setup Chat Template\n",
    "\n",
    "Configure the Gemma-3 chat template for proper conversation formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b094d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"gemma3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d566204",
   "metadata": {},
   "source": [
    "## Load GSM8K Dataset\n",
    "\n",
    "Load the GSM8K dataset which contains grade school math word problems with step-by-step solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c491408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd75466133b243fba440171e2d9375e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91acb1f8f3e24d4ab6552327165cb042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82b26e74fc24a889586bb54e11460ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8e28657acf4b979622badfe4a9ebe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6adc32d8555e455e8e17884212cc7dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 7473 examples\n",
      "\n",
      "Sample problem:\n",
      "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "\n",
      "Answer: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load UltraChat conversation dataset\n",
    "# Using train_sft split which is curated for supervised fine-tuning\n",
    "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft[:10000]\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"\\nSample conversation:\")\n",
    "print(f\"Messages: {dataset[0]['messages'][:2]}...\")  # Show first 2 turns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071aeee",
   "metadata": {},
   "source": [
    "## Format Dataset for Training\n",
    "\n",
    "Convert UltraChat conversations to the Gemma-3 chat format. The dataset already has proper user/assistant turn structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac17314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48b32d97d9547ee9e248345f5a43ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted example:\n",
      "[{'content': 'You are a helpful math tutor. Solve the given math problem step by step.\\nShow your reasoning clearly and provide the final numerical answer at the end.\\nFormat your final answer as: #### [number]\\n\\nProblem: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'role': 'user'}, {'content': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "def convert_ultrachat_to_gemma(example):\n",
    "    \"\"\"Convert UltraChat messages to Gemma-3 conversation format.\"\"\"\n",
    "    conversations = []\n",
    "    for msg in example['messages']:\n",
    "        role = msg['role']\n",
    "        # Map 'assistant' to 'model' for Gemma format if needed\n",
    "        if role == 'system':\n",
    "            # Prepend system message to first user message or skip\n",
    "            continue\n",
    "        conversations.append({\n",
    "            \"role\": role if role == \"user\" else \"assistant\",\n",
    "            \"content\": msg['content']\n",
    "        })\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "# Convert dataset\n",
    "dataset = dataset.map(convert_ultrachat_to_gemma)\n",
    "\n",
    "# Preview converted example\n",
    "\n",
    "print(\"Converted example:\")    print(turn['content'][:200] + \"...\" if len(turn['content']) > 200 else turn['content'])\n",
    "\n",
    "for i, turn in enumerate(dataset[0][\"conversations\"][:4]):    print(f\"\\nTurn {i+1} ({turn['role']}):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d722507",
   "metadata": {},
   "source": [
    "## Apply Chat Template\n",
    "\n",
    "Apply the Gemma-3 chat template to format conversations for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fa6d6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bc62790c384d63893a93a9a50f9014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted training example:\n",
      "<start_of_turn>user\n",
      "You are a helpful math tutor. Solve the given math problem step by step.\n",
      "Show your reasoning clearly and provide the final numerical answer at the end.\n",
      "Format your final answer as: #### [number]\n",
      "\n",
      "Problem: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 c...\n"
     ]
    }
   ],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Apply chat template to conversations.\"\"\"\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ).removeprefix('<bos>')\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Preview formatted text\n",
    "print(\"Formatted training example:\")\n",
    "print(dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a5761",
   "metadata": {},
   "source": [
    "## Configure Training\n",
    "\n",
    "Set up the SFT trainer with optimized hyperparameters for conversational fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b5310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Switching to float32 training since model cannot work with float16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd7ebf19e9d4f988d956479ee0e6465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=None,\n",
    "    args=SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=2,  # Effective batch size = 8\n",
    "        warmup_steps=10,\n",
    "        max_steps=200,  # Adjust for full training: num_train_epochs=1\n",
    "        learning_rate=5e-5,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs_conversation\",\n",
    "        report_to=\"none\",  # Use \"wandb\" for Weights & Biases logging\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de46c71d",
   "metadata": {},
   "source": [
    "## Enable Response-Only Training\n",
    "\n",
    "Train only on the assistant's responses (math solutions), not on the user's questions. This improves training efficiency and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea1e1ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4914e3990f8e47b4b86373811dfbc4ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=6):   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<start_of_turn>user\\n\",\n",
    "    response_part=\"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6aa2f",
   "metadata": {},
   "source": [
    "## Verify Training Masking\n",
    "\n",
    "Confirm that only the model's responses are being trained on (unmasked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1068f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full input:\n",
      "<bos><start_of_turn>user\n",
      "You are a helpful math tutor. Solve the given math problem step by step.\n",
      "Show your reasoning clearly and provide the final numerical answer at the end.\n",
      "Format your final answer as: #### [number]\n",
      "\n",
      "Problem: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>...\n",
      "\n",
      "==================================================\n",
      "\n",
      "Training target (only assistant response):\n",
      "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72<end_of_turn>\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Show full input\n",
    "print(\"Full input:\")\n",
    "print(tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])[:500] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Show what the model is trained on (masked version)\n",
    "print(\"Training target (only assistant response):\")\n",
    "masked_labels = [tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]]\n",
    "print(tokenizer.decode(masked_labels).replace(tokenizer.pad_token, \"\")[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699408c",
   "metadata": {},
   "source": [
    "## Check Memory Usage\n",
    "\n",
    "Display current GPU memory statistics before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "984ba7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: Tesla T4\n",
      "Max memory: 14.741 GB\n",
      "Reserved memory: 0.832 GB\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU: {gpu_stats.name}\")\n",
    "print(f\"Max memory: {max_memory} GB\")\n",
    "print(f\"Reserved memory: {start_gpu_memory} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f894d678",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Start the fine-tuning process. This will train the model on conversational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d586d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 7,473 | Num Epochs = 1 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 30,375,936 of 298,474,112 (10.18% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 02:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.681900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.919100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.788100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.815600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.812400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.786200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.766300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.727400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.750600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.775400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.768900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.735900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.789900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.735100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.766500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.795500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.758000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.735600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b0dd4",
   "metadata": {},
   "source": [
    "## Training Statistics\n",
    "\n",
    "Display final memory usage and training time statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "290c0393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 182.16 seconds\n",
      "Training time: 3.04 minutes\n",
      "Peak reserved memory: 3.234 GB\n",
      "Peak memory for training: 2.402 GB\n",
      "Peak memory % of max: 21.939%\n",
      "Training memory % of max: 16.295%\n"
     ]
    }
   ],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']/60:.2f} minutes\")\n",
    "print(f\"Peak reserved memory: {used_memory} GB\")\n",
    "print(f\"Peak memory for training: {used_memory_for_lora} GB\")\n",
    "print(f\"Peak memory % of max: {used_percentage}%\")\n",
    "print(f\"Training memory % of max: {lora_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad042b",
   "metadata": {},
   "source": [
    "## Inference - Test the Model\n",
    "\n",
    "Test the fine-tuned model on a conversation to see how it responds to various queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36129fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input prompt:\n",
      "<start_of_turn>user\n",
      "You are a helpful math tutor. Solve the given math problem step by step.\n",
      "Show your reasoning clearly and provide the final numerical answer at the end.\n",
      "Format your final answer as: #### [number]\n",
      "\n",
      "Problem: A craft store makes a third of its sales in the fabric section, a quarter of its sales in the jewelry section, and the rest in the stationery section. They made 36 sales today. How many sales were in the stationery section?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model response:\n"
     ]
    }
   ],
   "source": [
    "# Test with a sample conversation from the dataset\n",
    "test_conversation = dataset[100]['conversations'][0]['content'] if dataset[100]['conversations'] else \"Hello, how are you?\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": test_conversation\n",
    "    }\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ").removeprefix('<bos>')\n",
    "\n",
    "print(\"Input prompt:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Model response:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50934e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The craft store made 36 x 12 = <<36*12=412>>412 sales in the fabric section.\n",
      "The craft store made 412 / 3 = <<412/3=133>>133 sales in the jewelry section.\n",
      "The craft store made 133 - 36 - 412 = <<133-36-412=1458>>1458 sales in the stationery section.\n",
      "#### 1458<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# Generate response with streaming\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=64,\n",
    "    streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d1da9",
   "metadata": {},
   "source": [
    "## Test with Custom Conversations\n",
    "\n",
    "Try the model with various conversation topics to evaluate its conversational abilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8decce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom problem response:\n",
      "On Tuesday, the bakery sold 60 cupcakes - 45 cupcakes = <<60-45=15>>15 cupcakes.\n",
      "The total number of cupcakes sold on Tuesday is 15 cupcakes + 80 cupcakes = <<15+80=95>>95 cupcakes.\n",
      "The total number of cookies sold on Tuesday is 15 cupcakes + 55 cupcakes = <<15+55=70>>70 cupcakes.\n",
      "The total number of cupcakes sold on Tuesday is 95 cupcakes + 70 cupcakes = <<95+70=165>>165 cupcakes.\n",
      "The total number of cupcakes sold on Tuesday is 165 cupcakes + 3 cupcakes = <<165+3=168>>168 cupcakes.\n",
      "The total number of cupcakes sold on Tuesday is 168 cupcakes + 3 cupcakes = <<168+3=171>>171 cupcakes.\n",
      "The total number of cupcakes sold on Tuesday is 171 cupcakes + 2 cupcakes = <<171+2=173>>173 cupcakes.\n",
      "The total number of cupcakes sold on Tuesday is 173 cupcakes + 2 cupcakes = <<173+2=175>>175 cupcakes.\n",
      "The total number of cupcakes sold on Tuesday is 175 cupcakes + 2 cupcakes = <<175+2=177>>177 cupcakes.\n",
      "The total number of cupcakes sold on Tuesday is 177 cupcakes + 2 cupcakes = <<177+2=181>>181 cupcakes.\n",
      "The total number of cupcakes sold on Tuesday is 181 cupcakes + 2 cupcakes = <<181+2=183>>183 cupcakes.\n",
      "The total number of cupcakes sold on Tuesday is 183 cupcakes + 2 cupcakes = <<183+2=185>>185 cupcakes.\n",
      "On Tuesday, the bakery sold 15 cupcakes + 165 cupcakes = <<15+165=180>>180 cupcakes.\n",
      "The total revenue made over the two days is 180 cupcakes * $3/cupcake = $180 * $3 = $540.\n",
      "#### 540<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "# Test various conversation topics\n",
    "test_prompts = [\n",
    "    \"What are some tips for learning a new programming language?\",\n",
    "    \"Can you explain the difference between machine learning and deep learning?\",\n",
    "    \"Write a short poem about artificial intelligence.\",\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Test {i+1}: {prompt}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    ).removeprefix('<bos>')\n",
    "    \n",
    "    _ = model.generate(\n",
    "        **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        top_k=64,\n",
    "        streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a53035f",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Save the fine-tuned LoRA adapters locally. You can also push to Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters locally\n",
    "model.save_pretrained(\"gemma3-270m-conversation-lora\")\n",
    "tokenizer.save_pretrained(\"gemma3-270m-conversation-lora\")\n",
    "\n",
    "print(\"Model saved to: gemma3-270m-conversation-lora/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to push to Hugging Face Hub\n",
    "# model.push_to_hub(\"your_username/gemma3-270m-conversation-lora\", token=\"hf_...\")\n",
    "# tokenizer.push_to_hub(\"your_username/gemma3-270m-conversation-lora\", token=\"hf_...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123dd7fd",
   "metadata": {},
   "source": [
    "## Save Merged Model (Optional)\n",
    "\n",
    "Merge LoRA adapters with base model and save in different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec827998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 16bit (for VLLM deployment)\n",
    "if False:  # Change to True to save\n",
    "    model.save_pretrained_merged(\n",
    "        \"gemma3-270m-conversation-merged\",\n",
    "        tokenizer,\n",
    "        save_method=\"merged_16bit\"\n",
    "    )\n",
    "\n",
    "# Save as GGUF for llama.cpp\n",
    "if False:  # Change to True to save\n",
    "    model.save_pretrained_gguf(\n",
    "        \"gemma3-270m-conversation-gguf\",\n",
    "        tokenizer,\n",
    "        quantization_method=\"Q8_0\",  # Q8_0, BF16, or F16\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d6bfe3",
   "metadata": {},
   "source": [
    "## Load Saved Model (Optional)\n",
    "\n",
    "Load the saved LoRA adapters for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e501c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:  # Change to True to load saved model\n",
    "    from unsloth import FastModel\n",
    "    \n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name=\"gemma3-270m-conversation-lora\",\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=False,\n",
    "    )\n",
    "    print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e45f4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Loading** the Gemma-3 270M model with Unsloth optimizations\n",
    "2. **Configuring** LoRA adapters for efficient fine-tuning\n",
    "3. **Preparing** the UltraChat conversation dataset for training\n",
    "4. **Training** with response-only masking for improved accuracy\n",
    "5. **Testing** the model on various conversation topics\n",
    "6. **Saving** the fine-tuned model in various formats\n",
    "\n",
    "### Dataset Used\n",
    "\n",
    "- **HuggingFaceH4/ultrachat_200k** - A high-quality multi-turn conversation dataset\n",
    "- Contains diverse topics and helpful AI assistant responses\n",
    "- Great for training general-purpose conversational abilities\n",
    "\n",
    "\n",
    "### Next Steps- Deploy with VLLM or llama.cpp for production inference\n",
    "\n",
    "- Fine-tune on domain-specific conversations for specialized assistants\n",
    "\n",
    "- Increase `max_steps` or use `num_train_epochs=1` for full training- Try other conversation datasets like `OpenAssistant/oasst1` or `databricks/dolly-15k`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
