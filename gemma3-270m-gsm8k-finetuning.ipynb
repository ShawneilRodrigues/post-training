{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7cd1ed",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install required packages for Unsloth and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ada8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.56.2\n",
    "!pip install --no-deps trl==0.22.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb0e06",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Load the Gemma-3 270M model using Unsloth's FastModel for efficient inference and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2225abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048  # Sufficient for GSM8K problems\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-3-270m-it\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,   # Full precision for small model\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,  # Use LoRA for efficiency\n",
    "    # token = \"hf_...\",  # Uncomment if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a5b7b4",
   "metadata": {},
   "source": [
    "## Configure LoRA Adapters\n",
    "\n",
    "Add LoRA adapters for parameter-efficient fine-tuning. This allows us to update only a small fraction of the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57e6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    r=128,  # LoRA rank - higher values = more capacity but more memory\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0,  # No dropout for optimized training\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",  # 30% less VRAM\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2e7ac4",
   "metadata": {},
   "source": [
    "## Setup Chat Template\n",
    "\n",
    "Configure the Gemma-3 chat template for proper conversation formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b094d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"gemma3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d566204",
   "metadata": {},
   "source": [
    "## Load GSM8K Dataset\n",
    "\n",
    "Load the GSM8K dataset which contains grade school math word problems with step-by-step solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c491408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load GSM8K training split\n",
    "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"\\nSample problem:\")\n",
    "print(f\"Question: {dataset[0]['question']}\")\n",
    "print(f\"\\nAnswer: {dataset[0]['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1071aeee",
   "metadata": {},
   "source": [
    "## Format Dataset for Training\n",
    "\n",
    "Convert GSM8K examples to the Gemma-3 chat format with system instructions for mathematical reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac17314",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful math tutor. Solve the given math problem step by step.\n",
    "Show your reasoning clearly and provide the final numerical answer at the end.\n",
    "Format your final answer as: #### [number]\"\"\"\n",
    "\n",
    "def convert_gsm8k_to_chat(example):\n",
    "    \"\"\"Convert GSM8K example to Gemma-3 chat format.\"\"\"\n",
    "    return {\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"user\", \"content\": f\"{SYSTEM_PROMPT}\\n\\nProblem: {example['question']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": example['answer']}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Convert dataset\n",
    "dataset = dataset.map(convert_gsm8k_to_chat)\n",
    "\n",
    "# Preview converted example\n",
    "print(\"Converted example:\")\n",
    "print(dataset[0][\"conversations\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d722507",
   "metadata": {},
   "source": [
    "## Apply Chat Template\n",
    "\n",
    "Apply the Gemma-3 chat template to format conversations for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Apply chat template to conversations.\"\"\"\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [\n",
    "        tokenizer.apply_chat_template(\n",
    "            convo,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        ).removeprefix('<bos>')\n",
    "        for convo in convos\n",
    "    ]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Preview formatted text\n",
    "print(\"Formatted training example:\")\n",
    "print(dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a5761",
   "metadata": {},
   "source": [
    "## Configure Training\n",
    "\n",
    "Set up the SFT trainer with optimized hyperparameters for math reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b5310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=None,\n",
    "    args=SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=2,  # Effective batch size = 8\n",
    "        warmup_steps=10,\n",
    "        max_steps=200,  # Adjust for full training: num_train_epochs=1\n",
    "        learning_rate=5e-5,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs_gsm8k\",\n",
    "        report_to=\"none\",  # Use \"wandb\" for Weights & Biases logging\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de46c71d",
   "metadata": {},
   "source": [
    "## Enable Response-Only Training\n",
    "\n",
    "Train only on the assistant's responses (math solutions), not on the user's questions. This improves training efficiency and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e1ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part=\"<start_of_turn>user\\n\",\n",
    "    response_part=\"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6aa2f",
   "metadata": {},
   "source": [
    "## Verify Training Masking\n",
    "\n",
    "Confirm that only the model's responses are being trained on (unmasked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068f726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show full input\n",
    "print(\"Full input:\")\n",
    "print(tokenizer.decode(trainer.train_dataset[0][\"input_ids\"])[:500] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Show what the model is trained on (masked version)\n",
    "print(\"Training target (only assistant response):\")\n",
    "masked_labels = [tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[0][\"labels\"]]\n",
    "print(tokenizer.decode(masked_labels).replace(tokenizer.pad_token, \"\")[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c699408c",
   "metadata": {},
   "source": [
    "## Check Memory Usage\n",
    "\n",
    "Display current GPU memory statistics before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984ba7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU: {gpu_stats.name}\")\n",
    "print(f\"Max memory: {max_memory} GB\")\n",
    "print(f\"Reserved memory: {start_gpu_memory} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f894d678",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Start the fine-tuning process. This will train the model on GSM8K math problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d586d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b0dd4",
   "metadata": {},
   "source": [
    "## Training Statistics\n",
    "\n",
    "Display final memory usage and training time statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c0393",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']/60:.2f} minutes\")\n",
    "print(f\"Peak reserved memory: {used_memory} GB\")\n",
    "print(f\"Peak memory for training: {used_memory_for_lora} GB\")\n",
    "print(f\"Peak memory % of max: {used_percentage}%\")\n",
    "print(f\"Training memory % of max: {lora_percentage}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ad042b",
   "metadata": {},
   "source": [
    "## Inference - Test the Model\n",
    "\n",
    "Test the fine-tuned model on a math problem to see if it learned to solve GSM8K-style questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36129fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample problem from the dataset\n",
    "test_problem = dataset[100]['question'] if 'question' in dataset[100] else dataset[100]['conversations'][0]['content']\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{SYSTEM_PROMPT}\\n\\nProblem: {test_problem}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ").removeprefix('<bos>')\n",
    "\n",
    "print(\"Input prompt:\")\n",
    "print(text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Model response:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50934e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# Generate response with streaming\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=64,\n",
    "    streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d1da9",
   "metadata": {},
   "source": [
    "## Test with a Custom Problem\n",
    "\n",
    "Try the model with a new math problem to evaluate generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8decce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_problem = \"\"\"A bakery sells cupcakes for $3 each and cookies for $2 each. \n",
    "On Monday, they sold 45 cupcakes and 80 cookies. \n",
    "On Tuesday, they sold 60 cupcakes and 55 cookies. \n",
    "How much total revenue did the bakery make over these two days?\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{SYSTEM_PROMPT}\\n\\nProblem: {custom_problem}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ").removeprefix('<bos>')\n",
    "\n",
    "print(\"Custom problem response:\")\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=64,\n",
    "    streamer=TextStreamer(tokenizer, skip_prompt=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a53035f",
   "metadata": {},
   "source": [
    "## Save the Model\n",
    "\n",
    "Save the fine-tuned LoRA adapters locally. You can also push to Hugging Face Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d7a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters locally\n",
    "model.save_pretrained(\"gemma3-270m-gsm8k-lora\")\n",
    "tokenizer.save_pretrained(\"gemma3-270m-gsm8k-lora\")\n",
    "\n",
    "print(\"Model saved to: gemma3-270m-gsm8k-lora/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda8d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to push to Hugging Face Hub\n",
    "# model.push_to_hub(\"your_username/gemma3-270m-gsm8k-lora\", token=\"hf_...\")\n",
    "# tokenizer.push_to_hub(\"your_username/gemma3-270m-gsm8k-lora\", token=\"hf_...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123dd7fd",
   "metadata": {},
   "source": [
    "## Save Merged Model (Optional)\n",
    "\n",
    "Merge LoRA adapters with base model and save in different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec827998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge to 16bit (for VLLM deployment)\n",
    "if False:  # Change to True to save\n",
    "    model.save_pretrained_merged(\n",
    "        \"gemma3-270m-gsm8k-merged\",\n",
    "        tokenizer,\n",
    "        save_method=\"merged_16bit\"\n",
    "    )\n",
    "\n",
    "# Save as GGUF for llama.cpp\n",
    "if False:  # Change to True to save\n",
    "    model.save_pretrained_gguf(\n",
    "        \"gemma3-270m-gsm8k-gguf\",\n",
    "        tokenizer,\n",
    "        quantization_method=\"Q8_0\",  # Q8_0, BF16, or F16\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d6bfe3",
   "metadata": {},
   "source": [
    "## Load Saved Model (Optional)\n",
    "\n",
    "Load the saved LoRA adapters for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e501c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:  # Change to True to load saved model\n",
    "    from unsloth import FastModel\n",
    "    \n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name=\"gemma3-270m-gsm8k-lora\",\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=False,\n",
    "    )\n",
    "    print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5e45f4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Loading** the Gemma-3 270M model with Unsloth optimizations\n",
    "2. **Configuring** LoRA adapters for efficient fine-tuning\n",
    "3. **Preparing** the GSM8K dataset for math reasoning training\n",
    "4. **Training** with response-only masking for improved accuracy\n",
    "5. **Testing** the model on math word problems\n",
    "6. **Saving** the fine-tuned model in various formats\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Increase `max_steps` or use `num_train_epochs=1` for full training\n",
    "- Evaluate on GSM8K test set for benchmark comparison\n",
    "- Try GRPO training for reinforcement learning from preferences\n",
    "- Deploy with VLLM or llama.cpp for production inference"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
